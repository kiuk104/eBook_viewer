import { GoogleGenerativeAI } from "@google/generative-ai";
import { getGeminiKey } from './settings.js';

/**
 * í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ì•ˆì „í•˜ê²Œ ìë¥´ëŠ” í•¨ìˆ˜
 */
function chunkText(text, maxLength) {
    const chunks = [];
    let currentChunk = "";
    const paragraphs = text.split(/\n\s*\n/); // ë¹ˆ ì¤„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ ë¶„ë¦¬

    for (const paragraph of paragraphs) {
        if (paragraph.length > maxLength) {
            if (currentChunk) {
                chunks.push(currentChunk);
                currentChunk = "";
            }
            for (let i = 0; i < paragraph.length; i += maxLength) {
                chunks.push(paragraph.slice(i, i + maxLength));
            }
        } else {
            if (currentChunk.length + paragraph.length + 2 < maxLength) {
                currentChunk += (currentChunk ? "\n\n" : "") + paragraph;
            } else {
                chunks.push(currentChunk);
                currentChunk = paragraph;
            }
        }
    }
    if (currentChunk) chunks.push(currentChunk);
    return chunks;
}

export async function cleanTextWithAI(text, onProgress) {
    const apiKey = getGeminiKey();
    if (!apiKey) {
        alert("ì„¤ì •ì—ì„œ Google Gemini API í‚¤ë¥¼ ë¨¼ì € ë“±ë¡í•´ì£¼ì„¸ìš”!");
        throw new Error("API Key missing");
    }

    const genAI = new GoogleGenerativeAI(apiKey);
    const CHUNK_SIZE = 6000; 
    const chunks = chunkText(text, CHUNK_SIZE);
    const totalChunks = chunks.length;
    let combinedResult = "";

    console.log(`ğŸ“ í…ìŠ¤íŠ¸ ë¶„í•  ì²˜ë¦¬ ì‹œì‘: ì´ ${totalChunks}ê°œ êµ¬ì—­`);

    const modelsToTry = [
        "gemini-2.0-flash-exp",
        "gemini-1.5-flash-002",
        "gemini-1.5-flash"
    ];

    for (let i = 0; i < totalChunks; i++) {
        const chunk = chunks[i];
        const isFirstChunk = (i === 0);
        
        if (onProgress) {
            const percent = Math.round(((i + 1) / totalChunks) * 100);
            onProgress(`AI ë³€í™˜ ì¤‘... (${i + 1}/${totalChunks} êµ¬ì—­, ${percent}%)`);
        }

        // [ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸] êµ¬ì¡°í™” ì§€ì‹œ ê°•í™”
        let systemPrompt = "";
        if (isFirstChunk) {
            systemPrompt = `
            ë‹¹ì‹ ì€ ì „ë¬¸ ì „ìì±… í¸ì§‘ìì…ë‹ˆë‹¤.
            
            [í•„ìˆ˜ ê·œì¹™]
            1. **ë©”ì¸ ì œëª©(H1)**: í…ìŠ¤íŠ¸ì—ì„œ **ì±…ì˜ ì‹¤ì œ ì œëª©**ë§Œ ì¶”ì¶œí•˜ì—¬ ìµœìƒë‹¨ì— '# ' (H1 íƒœê·¸)ë¥¼ ë¶™ì—¬ ì‘ì„±í•˜ì„¸ìš”.
               - âŒ 'ì œëª©'ì´ë¼ëŠ” ê¸€ìëŠ” ì“°ì§€ ë§ˆì„¸ìš”. (ì˜ˆ: '# íƒˆëª…ê²€')
            
            2. **ì„œì§€ ì •ë³´(H3)**: ì œëª© ì•„ë˜ì— ì €ì, ì¶œíŒì‚¬ ì •ë³´ë¥¼ **í‘œ(Table) ëŒ€ì‹  '### '(H3 íƒœê·¸)**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”.
               - í˜•ì‹ ì˜ˆì‹œ:
                 ### ì €ì : í™ê¸¸ë™
                 ### ì¶œíŒì‚¬ : ëŒ€í•œì¶œíŒì‚¬
            
            3. **êµ¬ë¶„ì„ (í•„ìˆ˜)**: ì„œì§€ ì •ë³´ê°€ ëë‚œ í›„, ë³¸ë¬¸ì´ ì‹œì‘ë˜ê¸° ì „ì— ë°˜ë“œì‹œ '---' (ê°€ë¡œì„ )ì„ í•œ ì¤„ ë„£ì–´ì£¼ì„¸ìš”.

            4. **ì¸ìš©êµ¬/ë…ë°±**: ì¤‘ìš”í•œ ë…ë°±ì´ë‚˜ ì¸ìš©êµ¬ëŠ” '>' ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ë¶„í•˜ì„¸ìš”.
            5. **ë³¸ë¬¸ ìŠ¤íƒ€ì¼**: ë¬¸ë‹¨ ì‚¬ì´ì—ëŠ” ë°˜ë“œì‹œ ë¹ˆ ì¤„ì„ ë„£ì–´ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”.
            `;
        } else {
            // ... (else ë¸”ë¡, ì¦‰ ë’·ë¶€ë¶„ ì²­ì†Œ ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€)
             systemPrompt = `
            ë‹¹ì‹ ì€ ì†Œì„¤ í¸ì§‘ìì…ë‹ˆë‹¤. ì•ë¶€ë¶„ì— ì´ì–´ì§€ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬ ì¤‘ì…ë‹ˆë‹¤.
            
            [í•„ìˆ˜ ê·œì¹™]
            1. **ë©”íƒ€ë°ì´í„° ê¸ˆì§€**: í‘œë‚˜ ì œëª©ì€ ì ˆëŒ€ ë‹¤ì‹œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”. ë³¸ë¬¸ë§Œ ì´ì–´ì„œ ì‘ì„±í•˜ì„¸ìš”.
            2. **ìŠ¤íƒ€ì¼**:
               - ì¤‘ìš”í•œ ë…ë°±ì´ë‚˜ ê°•ì¡°í•  ë¬¸ì¥ì€ '>' ê¸°í˜¸(ì¸ìš©êµ¬)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
               - ë¬¸ë‹¨ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ì„ ë„£ìœ¼ì„¸ìš”.
            3. **ë‚´ìš© ìœ ì§€**: ë‚´ìš©ì€ ëŠê¹€ ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.
            `;
        }      
        let chunkSuccess = false;
        let chunkResult = "";

        for (const modelName of modelsToTry) {
            try {
                const model = genAI.getGenerativeModel({ 
                    model: modelName,
                    systemInstruction: systemPrompt 
                });

                const result = await model.generateContent({
                    contents: [{ role: "user", parts: [{ text: chunk }] }],
                    generationConfig: {
                        maxOutputTokens: 8192,
                        temperature: 0.7,
                    }
                });

                const response = await result.response;
                chunkResult = response.text();
                
                if (chunkResult) {
                    chunkSuccess = true;
                    combinedResult += chunkResult + "\n\n";
                    break; 
                }
            } catch (error) {
                console.warn(`âš ï¸ [Chunk ${i+1}] ${modelName} ì‹¤íŒ¨:`, error);
            }
        }

        if (!chunkSuccess) {
            alert(`ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (${i+1}ë²ˆì§¸ êµ¬ì—­)\në„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.`);
            throw new Error("Chunk processing failed");
        }
        
        await new Promise(r => setTimeout(r, 500));
    }

    // í›„ì²˜ë¦¬: ê³¼ë„í•œ ê³µë°±ë§Œ ì œê±° (ê¸°í˜¸ëŠ” ì‚´ë ¤ë‘ )
    combinedResult = combinedResult.replace(/\n{3,}/g, "\n\n");

    return combinedResult;
}