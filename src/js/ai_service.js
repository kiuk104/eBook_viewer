import { GoogleGenerativeAI } from "@google/generative-ai";
import { getGeminiKey } from './settings.js';

/**
 * í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ì•ˆì „í•˜ê²Œ ìë¥´ëŠ” í•¨ìˆ˜
 */
function chunkText(text, maxLength) {
    const chunks = [];
    let currentChunk = "";
    const paragraphs = text.split(/\n\s*\n/); // ë¹ˆ ì¤„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ ë¶„ë¦¬

    for (const paragraph of paragraphs) {
        if (paragraph.length > maxLength) {
            if (currentChunk) {
                chunks.push(currentChunk);
                currentChunk = "";
            }
            for (let i = 0; i < paragraph.length; i += maxLength) {
                chunks.push(paragraph.slice(i, i + maxLength));
            }
        } else {
            if (currentChunk.length + paragraph.length + 2 < maxLength) {
                currentChunk += (currentChunk ? "\n\n" : "") + paragraph;
            } else {
                chunks.push(currentChunk);
                currentChunk = paragraph;
            }
        }
    }
    if (currentChunk) chunks.push(currentChunk);
    return chunks;
}

export async function cleanTextWithAI(text, onProgress) {
    const apiKey = getGeminiKey();
    if (!apiKey) {
        alert("ì„¤ì •ì—ì„œ Google Gemini API í‚¤ë¥¼ ë¨¼ì € ë“±ë¡í•´ì£¼ì„¸ìš”!");
        throw new Error("API Key missing");
    }

    const genAI = new GoogleGenerativeAI(apiKey);
    const CHUNK_SIZE = 6000; 
    const chunks = chunkText(text, CHUNK_SIZE);
    const totalChunks = chunks.length;
    let combinedResult = "";

    console.log(`ğŸ“ í…ìŠ¤íŠ¸ ë¶„í•  ì²˜ë¦¬ ì‹œì‘: ì´ ${totalChunks}ê°œ êµ¬ì—­`);

    const modelsToTry = [
        "gemini-2.0-flash",
        "gemini-2.0-flash-001",
        "gemini-2.5-flash" 
    ];

    for (let i = 0; i < totalChunks; i++) {
        const chunk = chunks[i];
        const isFirstChunk = (i === 0);
        
        if (onProgress) {
            const percent = Math.round(((i + 1) / totalChunks) * 100);
            onProgress(`AI ë³€í™˜ ì¤‘... (${i + 1}/${totalChunks} êµ¬ì—­, ${percent}%)`);
        }

        // [í•µì‹¬ ë¡œì§] ì²« ë²ˆì§¸ ì¡°ê°ê³¼ ë‚˜ë¨¸ì§€ ì¡°ê°ì˜ ê·œì¹™ì„ ë‹¤ë¥´ê²Œ ì„¤ì •
        let headerRule = "";
        if (isFirstChunk) {
            headerRule = `6. **ì œëª©/ì„œì§€ì •ë³´:** í…ìŠ¤íŠ¸ ì‹œì‘ ë¶€ë¶„ì— ì±… ì œëª©, ì €ì, ì±•í„°ëª…ì´ ìˆë‹¤ë©´ # (H1) ë˜ëŠ” ### (H3) í—¤ë”ë¡œ ì˜ˆì˜ê²Œ í¬ë§·íŒ…í•˜ì„¸ìš”.`;
        } else {
            headerRule = `6. **ì œëª© ë°˜ë³µ ê¸ˆì§€:** ì´ í…ìŠ¤íŠ¸ëŠ” ì†Œì„¤ì˜ ì¤‘ê°„ ë¶€ë¶„ì…ë‹ˆë‹¤. **ì±… ì œëª©, ì €ì, ì¶œíŒì‚¬ ì •ë³´ë¥¼ ì ˆëŒ€ ë‹¤ì‹œ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.** ë¬¸ë§¥ì„ ì´ì–´ë°›ì•„ ë°”ë¡œ ì†Œì„¤ ë‚´ìš©ë¶€í„° ì‹œì‘í•˜ì„¸ìš”.`;
        }

        // [ìˆ˜ì • ì™„ë£Œ] systemPromptë¥¼ ifë¬¸ ë°–ìœ¼ë¡œ êº¼ë‚´ì„œ ì •ìƒ ì‘ë™í•˜ê²Œ í•¨
        const systemPrompt = `
        ë‹¹ì‹ ì€ ì†Œì„¤ í…ìŠ¤íŠ¸ êµì • ë° í¬ë§·íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ì…ë ¥í•œ í…ìŠ¤íŠ¸ë¥¼ ì½ê¸° í¸í•œ **ë§ˆí¬ë‹¤ìš´(Markdown) ì†Œì„¤ í˜•ì‹**ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.

        [ğŸš¨ í•„ìˆ˜ ì ˆëŒ€ ê·œì¹™ ğŸš¨]
        1. **ì¸ìš©ë¬¸(>) ì‚¬ìš© ê¸ˆì§€:** ë³¸ë¬¸ì— ì ˆëŒ€ '>' ê¸°í˜¸ë¥¼ ë¶™ì´ì§€ ë§ˆì„¸ìš”. ê·¸ëƒ¥ í‰ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        2. **ì¤„ë°”ê¿ˆ ì •ë¦¬(Line Joining):** ë¬¸ì¥ ì¤‘ê°„ì— ê°•ì œë¡œ ëŠê¸´ ì¤„ë°”ê¿ˆì„ ëª¨ë‘ ì œê±°í•˜ê³ , í•˜ë‚˜ì˜ ê¸´ ë¬¸ë‹¨ìœ¼ë¡œ ì´ì–´ ë¶™ì´ì„¸ìš”. (ê°€ì¥ ì¤‘ìš”)
        3. **ë¬¸ë‹¨ ê°„ê²©:** ë¬¸ë‹¨ê³¼ ë¬¸ë‹¨ ì‚¬ì´ì—ëŠ” **ë¹ˆ ì¤„ì„ í•˜ë‚˜ë§Œ** ë„£ì–´ì„œ êµ¬ë¶„í•˜ì„¸ìš”.
        4. **ëŒ€í™”ë¬¸:** ëŒ€í™”ë¬¸("...")ì€ ì¤„ì„ ë°”ê¿”ì„œ í‘œí˜„í•˜ë˜, ì¸ìš©ë¬¸ ê¸°í˜¸ ì—†ì´ ì‘ì„±í•˜ì„¸ìš”.
        5. **ë‚´ìš© ìœ ì§€:** ì›ë¬¸ì˜ ë‚´ìš©ì€ ì¡°ì‚¬ í•˜ë‚˜ë„ ë¹ ëœ¨ë¦¬ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”. (ìš”ì•½ ê¸ˆì§€)
        ${headerRule}
        `;

        let chunkSuccess = false;
        let chunkResult = "";

        for (const modelName of modelsToTry) {
            try {
                const model = genAI.getGenerativeModel({ 
                    model: modelName,
                    systemInstruction: systemPrompt 
                });

                const result = await model.generateContent({
                    contents: [{ role: "user", parts: [{ text: chunk }] }],
                    generationConfig: {
                        maxOutputTokens: 8192,
                        temperature: 0.7,
                    }
                });

                const response = await result.response;
                chunkResult = response.text();
                
                if (chunkResult) {
                    chunkSuccess = true;
                    combinedResult += chunkResult + "\n\n";
                    break; 
                }
            } catch (error) {
                console.warn(`âš ï¸ [Chunk ${i+1}] ${modelName} ì‹¤íŒ¨:`, error);
            }
        }

        if (!chunkSuccess) {
            alert(`ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (${i+1}ë²ˆì§¸ êµ¬ì—­)\në„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.`);
            throw new Error("Chunk processing failed");
        }
        
        await new Promise(r => setTimeout(r, 500));
    }

    // í›„ì²˜ë¦¬: ê³¼ë„í•œ ê³µë°± ì œê±°
    combinedResult = combinedResult.replace(/\n{3,}/g, "\n\n");

    return combinedResult;
}